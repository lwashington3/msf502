\documentclass[
	lecture={15},
	title={Inference with Regression Models}
]{msf502notes}

\begin{document}

\setcounter{chapter}{14}
%<*Chapter-15>
\chapter{Inference \w\ Regression Models}\label{ch:inference-w-regression-models}
\begin{objectives}
	\item Conduct tests of individual significance.
	\item Conduct a test of joint significance.
	\item \sout{Conduct a general test of linear restrictions.}
	\item \sout{Calculate and interpret interval estimates for predictions.}
	\item Explain the role of the assumptions on the OLS estimators.
	\item Describe common violations of the assumptions and offer remedies.
\end{objectives}

\section{Tests of Significance}\label{sec:tests-of-significance}
\lo{Conduct tests of individual significance.}
\begin{itemize}
	\item With two explanatory variables to choose from, we can formulate three linear models:
	\[ \begin{aligned}
		\text{Model 1: Win}&= \beta_{0} + \beta_{1}\text{BA} + \epsilon\\
		\text{Model 2: Win}&= \beta_{0} + \beta_{1}\text{ERA} + \epsilon\\
		\text{Model 3: Win}&= \beta_{0} + \beta_{1}\text{BA} + \beta_{2}\text{ERA} + \epsilon\\
	\end{aligned} \]
\end{itemize}

\subsection{Tests of Individual Significance}\label{subsec:tests-of-individual-significance}
\begin{itemize}
	\item Consider our standard multiple regression model:~\eqref{eq:multiple-regression-model}.
	\item In general, we can test whether $\beta_{j}$ is equal to, greater than, or less than some hypothesized value $\beta_{j0}$.
	\item This test could have one of three forms:
\end{itemize}
\begin{table}[H]
	\centering
	\caption{Three Forms of Individual Significance}
	\label{tab:three-forms-of-individual-significance}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Two-tailed Test} & \textbf{Right-tailed Test} & \textbf{Left-tailed Test}\\
		\hline
		$H_{0}: \beta_{j} = \beta_{j0}$ & $H_{0}: \beta_{j} \leq \beta_{j0}$ & $H_{0}: \beta_{j} \geq \beta_{j0}$\\
		\hline
		$H_{A}: \beta_{j} \neq \beta_{j0}$ & $H_{A}: \beta_{j} > \beta_{j0}$ & $H_{A}: \beta_{j} < \beta_{j0}$\\
		\hline
	\end{tabular}
\end{table}

\subsection{The Test Statistic}\label{subsec:the-test-statistic}
\begin{itemize}
	\item The appropriate test statistic is:
	\begin{equation}
		t_{df} = \frac{b_{j} - \beta_{j0}}{s_{b_{j}}}
		\label{eq:regression-inference-test-statistic}
	\end{equation}
	\item $s_{b_{j}}$ is the standard error of the estimator $b_{j}$.
	\item The test statistic will follow a $t$-distribution \w\ degrees of freedom $df = n - k - 1$.
\end{itemize}

\subsection{Testing $\beta_{j}=0$}\label{subsec:testing-beta-j-0}
\begin{itemize}
	\item By far, the most common hypothesis test for an individual coefficient is to test whether its value differs from zero.
	\item To see why, consider our model:~\eqref{eq:multiple-regression-model}.
	\item If a coefficient is equal to zero, then it implies that the explanatory variable is not a significant predictor of the response variable.
\end{itemize}

\subsection{Computer-Generated Output}\label{subsec:computer-generated-output}
\begin{itemize}
	\item Virtually all statistical software will automatically report a test statistic and a $p$-value \w\ each coefficient estimate.
	\item These values can be used whether the regression coefficient differs from zero.
	\item To perform a one-sided test where the hypothesized value is zero, divide the computer-reported $p$-value in half.
	\item If we wish to test whether the coefficient differs from a non-zero value, we need to compute a new test statistic.
\end{itemize}

\subsection{Intervals for the Parameters}\label{subsec:intervals-for-the-parameters}
\begin{itemize}
	\item A confidence interval for the $\beta_{j}$ parameter can be constructed using the formula:
	\begin{equation}
		b_{j} \pm t_{\alpha/2, df} s_{b_{j}}
		\label{eq:regression-confidence-interval}
	\end{equation}
	\item This can also be used to perform the two-sided test to determine whether a coefficient differs from zero.
	\item For ERA, the interval of $[-0.15, -0.08]$ does not include 0, indicating ERA is a significant predictor.
\end{itemize}

\subsection{Test for a Non-Zero Slope}\label{subsec:test-for-a-non-zero-slope}
\begin{itemize}
	\item A capital asset pricing model follows the equation:
	\begin{equation}
		y = \alpha + \beta x + \epsilon
		\label{eq:non-zero-slope-test}
	\end{equation}
	where $y$ = the risk-adjusted return of an asset, $R - R_{f}$ and $x$ = the risk-adjusted return to the market $R_{M} - R_{f}$.
	\item The estimate of $\beta$ is called the investment's \emph{beta value}.
	\item A $\beta > 1$ implies the stock is ``aggressive'', while a $\beta < 1$ implies it is ``conservative''.
%	\item If the value of the test statistic is less than the critical value, we reject the null hypothesis
\end{itemize}

\subsection{Test of Joint Significance}\label{subsec:test-of-joint-significance}
\lo{Conduct a test of joint significance.}
\begin{itemize}
	\item In addition to conducting tests of individual significance, we may also want to test the joint significance of all $k$ variables at once.
	\item The competing hypotheses for a test of joint significance are:
	\[ \begin{aligned}
		H_{0}&: \beta_{1} = \beta_{2} = \dots = \beta_{k} = 0\\
		H_{A}&: \text{at least one } \beta_{j} \neq 0\\
	\end{aligned} \]
\end{itemize}

\subsection{The Test Statistic}\label{subsec:joint-significance-test-statistic}
\begin{itemize}
	\item The test statistic for a test of joint significant is
	\begin{equation}
		\begin{aligned}
			F_{(df_{1}, df_{2})} &= \frac{MSR}{MSE}\\
			&= \frac{\frac{SSR}{k}}{\frac{SSE}{n-k-1}}
		\end{aligned}
		\label{eq:joint-significance-test-statistic}
	\end{equation}
	where $MSR$ and $\hyperref[eq:mse-2]{MSE}$ are the mean regression sum of squares and the mean error sum of squares, respectively.
	\item The numerator degrees of freedom, $df_{1} = k$, while the denominator degrees of freedom, $df_{2} = n - k - 1$.
\end{itemize}

\section{A General Test of Linear Restrictions}\label{sec:a-general-test-of-linear-restrictions}
\lo{Conduct a general test of linear restrictions.}
\begin{itemize}
	\item The significance tests in the previous section can also be labeled tests of \textbf{linear restrictions}.
	\item For example, if we have $k=3$ explanatory variables, testing whether $\beta_{2} = \beta_{3} = 0$ is equivalent to testing whether to restrict the model to only $x_{1}$.
	\item In this section, we apply the $F$-test for any number of linear restrictions; the resulting $F$-test is often referred to as the \textbf{partial F}-test.
\end{itemize}

\subsection{Restricted and Unrestricted Models}\label{subsec:restricted-and-unrestricted-models}
\begin{itemize}
	\item To conduct the partial $F$-test, we estimate the model \w\ and \wo\ the restrictions.
	\item In the \emph{restricted model}, we do not estimate the coefficients that are restricted under the null hypothesis.
	\item The \emph{unrestricted model} is a complete model that imposes no restrictions on the coefficients.
	\item If restrictions are valid, then the restricted model's error sum of squares, $SSE_{R}$, will not be significantly larger than the unrestricted model's error sum of squares $SSE_{U}$.
\end{itemize}

\subsection{The Test Statistic}\label{subsec:the-partial-f-test-statistic}
\begin{itemize}
	\item The test statistic for a partial $F$-test can be computed as
	\begin{equation}
		F_{(df_{1}, df_{2})} = \frac{\frac{SSE_{R} - SSE_{U}}{df_{1}}}{\frac{SSE_{U}}{df_{2}}}
		\label{eq:partial-f-test}
	\end{equation}
	where the numerator degrees of freedom, $df_{1}$, equals the number of restrictions on the model, and the denominator degrees of freedom, $df_{2}$, equals $n - k - 1$.
	\item If the test statistic is greater than the critical value, then we reject the null hypothesis and the restrictions are not valid.
\end{itemize}

\subsection{Car Wash Example}\label{subsec:linear-restrictions-example}
\begin{itemize}
	\item A manager at a car wash company wants to determine which promotions improve sales.
	\item He has information on sales, price discounts, and advertising expenditures on Radio and Newspaper in 40 Missouri counties. (Columns = [Country; Sales (in \$1,000s); Discount (in \%); Radio (in \$1,000s); Newspaper (in \$1,000s)])
	\item More specifically, he would like to test whether either type of advertising impacts sales.
	To do so, we form the competing hypotheses as:
	\[ \begin{aligned}
		H_{0}&:\ \beta_{2} = \beta_{3} = 0\\
		H_{A}&:\ \text{At least one of the coefficients is nonzero.}
	\end{aligned} \]
	\item To conduct the test, we need to estimate a restricted model (R) and an unrestricted model (U):
	\[ \begin{aligned}
		\text{(R) Sales} &= \beta_{0} + \beta_{1}\text{Discount} + \epsilon\\
		\text{(U) Sales} &= \beta_{0} + \beta_{1}\text{Discount} + \beta_{2}\text{Radio} + \beta_{3}\text{Newspaper} + \epsilon
	\end{aligned} \]
	\item From the tables (which I'm not adding to this PDF), we can see that $SSE_{U} = 1208.1348$ while the $SSE_{R} = 2182.5649$.
	We can now proceed to compute the value of the test statistic.
	\item The number of restrictions, $df_{1}$, equals 2.
	The unrestricted model has $df_{2} = n - k - 1 = 40 - 3 - 1 = 36$ degrees of freedom.
	\item We compute the value of the test statistic as:
	\[ \begin{aligned}
		F_{2, 36} &= \frac{\frac{2182.5649 - 1208.1348}{2}}{\frac{1208.1348}{36}}\\
		&= \frac{487.2151}{33.5593}\\
		&= 14.52\\
	\end{aligned} \]
	\item Since $F_{(2, 36)} = 14.52$ is greater than the critical value $F_{0.05, (2, 36)} = 3.26$, we reject the null hypothesis and conclude the restrictions are not valid.
\end{itemize}

\section{Interval Estimates for Predictions}\label{sec:interval-estimates-for-predictions}
\lo{Calculate and interpret interval estimates for predictions.}
\begin{itemize}
	\item Once we have developed a regression model, we often want to use it to make predictions.
	\item From the introductory case, what would we predict for a team \w\ an earned run average of 4.00 and a batting average of 0.250?
	Plugging these values into the estimated equation, we find:
	\[ \hat{\text{Win}} = 0.13 + 3.28(0.250) - 0.12(4.00) = 0.47 \]
	\item But this is only a point estimate and ignores sampling error.
	We could also provide interval estimates.
\end{itemize}

\subsection{Two Types of Predictions}\label{subsec:two-types-of-predictions}
\begin{itemize}
	\item We will develop two types of interval estimates regarding $y$:
	\begin{enumerate}
		\item A confidence interval for the \emph{expected} value of $y$
		\item A prediction interval for an \emph{individual} value of $y$
	\end{enumerate}
	\item It is common to refer to the first as a confidence interval and the second as a prediction interval.
\end{itemize}

\subsection{The Confidence Interval}\label{subsec:the-linear-confidence-interval}
\begin{itemize}
	\item The point estimate of $E(y^{0})$ is just the $\hat{y}$ value:
	\begin{equation}
		\hat{y}^{0} = b_{0} + b_{1}x_{1}^{0} + b_{2}x_{2}^{0} + \dots + b_{k}x_{K}^{0}
		\label{eq:linear-point-estimate}
	\end{equation}
	\item The confidence interval, as always, includes the point estimate, plus or minus the margin of error:
	\begin{equation}
		\hat{y}^{0} \pm t_{\alpha/2, df} se(\hat{y}^{0})
		\label{eq:point-margin-of-error}
	\end{equation}
	\item The term $se(\hat{y}^{0})$ is the standard error of the prediction.
	Though difficult to compute by hand if there is more than one explanatory variable in the model, we will develop a procedure to compute it \w\ a statistical package.
\end{itemize}

\section{Model Assumptions and Common Violations}\label{sec:model-assumptions-and-common-violations} % page 37
\lo{Explain the role of the assumptions on the OLS estimators.}
\begin{itemize}
	\item The statistical properties of OLS estimator, as well as the validity of the testing procedures, depend on a number of assumptions.
	We discuss those assumptions now.
	\begin{enumerate}[label=\arabic*.]
		\item The model~\eqref{eq:multiple-regression-model} is linear in the $\beta$ parameters with an additive error $\epsilon$.
		\item Conditional on the $x_{1}, \dots, x_{k}$ values, the expected error is 0, thus:
		\begin{equation}
			E(y) = \beta_{0} + \beta_{1}x_{1} + \dots + \beta_{k}x_{k}
			\label{eq:ols-expected-value}
		\end{equation}
		\item There is no exact linear relationship among the $x_{1}, \dots, x_{k}$ values (no perfect \emph{multicollinearity}).
		\item The variance of the error term $\epsilon$ is the same for all $x_{1}, \dots, x_{k}$ values.
		We call this \emph{homoskedasticity}.
		\item The error term $\epsilon$ is uncorrelated across observations, conditional on the explanatory variables.
		There is no \emph{serial correlation} or \emph{autocorrelation}.
		\item The error term $\epsilon$ is not correlated \w\ any of the predictors $x_{1}, \dots, x_{k}$,
		In other words, there is no \emph{endogeneity}.
		\item The error term $\epsilon$ is normally distributed.
		This assumption allows us to do hypothesis testing.
		If normality is not true, the tests may not be valid.
	\end{enumerate}
\end{itemize}

\subsection{Checking the Assumptions}\label{subsec:checking-the-assumptions}
\begin{itemize}
	\item The true error terms $\epsilon$ cannot be observed \bc\ they exist only in the population.
	We can, however, look at the residuals, $e = y - \hat{y}$, where $\hat{y} = b_{0} + \sum_{i=1}^{k} b_{i}x_{i}$, for each observation.
	\item It is common to plot to residuals on the vertical axis and an explanatory variable on the horizontal axis.
	\item When estimating a regression in Excel, the dialog box that opens after choosing \textbf{Data $>$ Data Analysis $>$ Regression} allows us to select \textit{Residuals} and \emph{Residual Plots} options.
\end{itemize}

\subsection{Common Violation 1: The Model Suffers from Multicollinearity}\label{subsec:common-violation-1:-the-model-suffers-from-multicollinearity}
\lo{Describe common violations of the assumptions and offer remedies.}
\begin{itemize}
	\item Perfect multicollinearity exists when two or more $x$ variables have an exact linear relationship.
	\item For example, suppose the $x$ data includes total cost, fixed cost and variable cost.
	\item Other data sets may have a great degree of multicollinearity that is not perfect.
	\item In these cases, we may see a high $R^{2}$ coupled \w\ individually insignificant explanatory variables.
	Additionally, unintuitive result may be indicative.
	\item A sample correlation between explanatory variables that is $>0.80$ or $<-0.80$ suggests severe multicollinearity.
\end{itemize}

\subsubsection{Remedying Multicollinearity}\label{subsubsec:remedying-multicollinearity}
\begin{itemize}
	\item A good remedy may be simply drop one of the collinear variables if we can justify it as redundant.
	\item Alternatively, we could try to increase our sample size.
	\item Another option would be to try to transform our variables so that they are no longer collinear.
	\item Last, especially if we are interested only in maintaining a high predictive power, it may make sense to do nothing.
\end{itemize}

\subsection{Common Violation 2: The Error Term is Heteroskedastic}\label{subsec:common-violation-2:-the-error-term-is-heteroskedastic}
\begin{itemize}
	\item The variance of the error term changes for different values of at least one explanatory variable.
	\item Informal residual plots can gauge heteroskedasticity (display a marked pattern, such as increasing magnitude along an axis).
\end{itemize}

\subsubsection{Remedying Heteroskedastic}\label{subsubsec:remedying-heteroskedastic}
\begin{itemize}
	\item Heteroskedastic results in inefficient estimators and the hypothesis tests for significance are no longer valid.
	\item To get around the second problem, some researchers use OLS estimates along \w\ corrected standard errors, called White's standard errors.
	Many statistical packages have this option available, unfortunately the current version of Excel does not.
\end{itemize}

\subsection{Common Violation 3: The Error Term is Serially Correlated}\label{subsec:common-violation-3:-the-error-term-is-serially-correlated}
\begin{itemize}
	\item We assume that the error term is uncorrelated across observations when obtaining OLS estimates.
	\item But this often breaks down in time series data.
	\item Remedies are not easily accessibly using Excel.
\end{itemize}

\subsection{Common Violation 4: The Explanatory Variable is Endogenous}\label{subsec:common-violation-4:-the-explanatory-variable-is-endogenous}
\begin{itemize}
	\item Endogeneity in the regression model refers to the error term being correlated \w\ the explanatory variables.
	\item This commonly occurs due to an omitted explanatory variable.
	\item For example, a person's salary may be highly correlated \w\ that person's innate ability.
	But since we cannot included it, ability gets incorporated in the error term.
	If we try to predict salary by years of education, which may also be correlated \w\ innate ability, then we have an endogeneity problem.
	\item Endogeneity will result in biased estimators, and so is quite a serious problem.
	\item Unfortunately, endogeneity is difficult to fix.
	Most commonly, we would like to find an instrumental variable, one that is correlated \w\ the endogenous explanatory variable but uncorrelated \w\ the error term.
	But it may be difficult to find such a variable.
\end{itemize}

%</Chapter-15>
\end{document}